### only needs standardisation if measured in mm, not as 0/1 variable
#meant<-mean(SURVEYDATA$rain, na.rm = TRUE)
#sdt<-sd(SURVEYDATA$rain, na.rm = TRUE)
#SURVEYDATA$rain<-(SURVEYDATA$rain-meant)/sdt
### create array for each covariate
wind<-array(NA, dim=c(nsites,3,nyears))
rain<-array(NA, dim=c(nsites,3,nyears))
time<-array(NA, dim=c(nsites,3,nyears))
day<-array(NA, dim=c(nsites,3,nyears))
ACT<-array(NA, dim=c(nsites,3,nyears))				## REPLACED ON 2 MAY WITH RAINFALL AMOUNT
### fill in array for each covariate
for (y in 2011:YEAR){
obsC<-subset(SURVEYDATA, year==y)
y<-match(y,c(2011:YEAR))						## translates the year (2011, 2012, etc.) into consecutive number (1,2,...) for array dimensions
x<-obsC %>% dplyr::select(Point, Count, time) %>% tidyr::spread(key=Count, value=time) %>% dplyr::arrange(Point)
time[,,y]<-as.matrix(x[,2:4])
x<-obsC %>% dplyr::select(Point, Count, day) %>% tidyr::spread(key=Count, value=day) %>% dplyr::arrange(Point)
day[,,y]<-as.matrix(x[,2:4])
x<-obsC %>% dplyr::select(Point, Count, Wind) %>%
mutate(Wind=ifelse(Wind<3,0,1)) %>%
tidyr::spread(key=Count, value=Wind) %>% dplyr::arrange(Point)
wind[,,y]<-as.matrix(x[,2:4])
x<-obsC %>% dplyr::select(Point, Count, activity) %>% tidyr::spread(key=Count, value=activity) %>% dplyr::arrange(Point)
ACT[,,y]<-as.matrix(x[,2:4])
x<-obsC %>% dplyr::select(Point, Count, Rain) %>% tidyr::spread(key=Count, value=Rain) %>% dplyr::arrange(Point)
rain[,,y]<-as.matrix(x[,2:4])
}
###############################################################################
####   REPLACE ALL NA IN COVARIATES otherwise "undefined node" error    #######
###############################################################################
for (d in 1:nyears){							### replace missing dates with mean for each survey round in each year
ACT[is.na(ACT[,1,d]),1,d]<-mean(ACT[,1,d], na.rm=T)
ACT[is.na(ACT[,2,d]),2,d]<-mean(ACT[,2,d], na.rm=T)
ACT[is.na(ACT[,3,d]),3,d]<-mean(ACT[,3,d], na.rm=T)
time[is.na(time[,1,d]),1,d]<-mean(time[,1,d], na.rm=T)
time[is.na(time[,2,d]),2,d]<-mean(time[,2,d], na.rm=T)
time[is.na(time[,3,d]),3,d]<-mean(time[,3,d], na.rm=T)
day[is.na(day[,1,d]),1,d]<-mean(day[,1,d], na.rm=T)
day[is.na(day[,2,d]),2,d]<-mean(day[,2,d], na.rm=T)
day[is.na(day[,3,d]),3,d]<-mean(day[,3,d], na.rm=T)
wind[is.na(wind[,1,d]),1,d]<-0
wind[is.na(wind[,2,d]),2,d]<-0
wind[is.na(wind[,3,d]),3,d]<-0
rain[is.na(rain[,1,d]),1,d]<-0
rain[is.na(rain[,2,d]),2,d]<-0
rain[is.na(rain[,3,d]),3,d]<-0
}
######################################################################################
#############  WRITE THE NIMBLE MODEL AND SET INITS  ############################
######################################################################################
# Specify model in NIMBLE format
trend.model<-nimbleCode({
####  Priors ########
loglam~dunif(-5,5)          ##  mean abundance prior
trend~dunif(-10,10)         ##  trend prior
beta.elev~dunif(-5,5)
#beta.rain~dunif(-2,2)
beta.canopy~dunif(-5,5)
beta.treeheight~dunif(-5,5)
bwind~dunif(-5,5)   ## wind can only have negative effect on detection
brain~dunif(-5,5)   ## rain can only have negative effect on detection
btime~dunif(-5,5)
b2time~dunif(-5,5)
bday~dunif(-5,5)
bridge~dunif(-5,5)
bact~dunif(-5,5)
## SITE RANDOM EFFECT ##
for(i in 1:nsite){
lam.site[i]~dnorm(loglam,tau=tau.site)    ## site-specific random effect with hierarchical centering from Kery email 5 June 2018
}
tau.site<-1/(sigma.site*sigma.site)
sigma.site~dunif(0,2)
## YEAR RANDOM EFFECT FOR ABUNDANCE AND ANNUALLY VARYING DETECTION PROBABILITY ##
for(year in 1:nyear){
p0[year]~dunif(0.01,0.99)## detection probability
logitp0[year]<-log(p0[year]/(1-p0[year]))
lam.year[year]~dnorm(trend*primocc[year],tau=tau.year)    ## year-specific random effect with hierarchical centering from Kery email 5 June 2018
}
tau.lp<-1/(sigma.p*sigma.p)
sigma.p~dunif(0,10)
tau.year<-1/(sigma.year*sigma.year)
sigma.year~dunif(0,10)
######### State and observation models ##############
for(year in 1:nyear){
for(i in 1:nsite){
log(lambda[i,year])<- lam.year[year]+
#beta.rain*rain[year]+
beta.elev*elev[i]+
beta.treeheight*treeheight[i]+
beta.canopy*canopy[i]+
lam.site[i]
N[i,year]~dpois(lambda[i,year])
for(t in 1:nrep){
M[i,t,year]~dbin(p[i,t,year],N[i,year])
p[i,t,year] <- exp(lp[i,t,year])/(1+exp(lp[i,t,year]))
lp[i,t,year] ~ dnorm(mu.lp[i,t,year], tau=tau.lp)
mu.lp[i,t,year]<-logitp0[year] +
btime*time[i,t,year]+
b2time * pow(time[i,t,year], 2) +
bday*day[i,t,year]+
bridge*ridge[i]+
bwind*wind[i,t,year]+
brain*rain[i,t,year]+
bact*ACT[i,t,year]
}
}
### DERIVED PARAMETER FOR EACH YEAR ###
totalN[year]<-sum(N[1:nsite,year])
anndet[year]<-mean(p[1:nsite,1:nrep,year])
}
# Computation of fit statistic (Bayesian p-value)
# Fit statistic for observed data
# Also, generate replicate data and compute fit stats for them
for(year in 1:nyear){
for(i in 1:nsite){
for(t in 1:nrep){
# Actual data
eval[i,t,year] <-N[i,year]*p[i,t,year] # Expected value
sd.resi[i,t,year]<-sqrt(eval[i,t,year]*(1-p[i,t,year])) +0.5
E[i,t,year]<-(M[i,t,year]-eval[i,t,year])/ sd.resi[i,t,year]
E2[i,t,year] <- pow(E[i,t,year],2)
# Replicate data sets
M.new[i,t,year]~dbin(p[i,t,year],N[i,year])
E.new[i,t,year]<-(M.new[i,t,year]-eval[i,t,year])/sd.resi[i,t,year]
E2.new[i,t,year] <- pow(E.new[i,t,year], 2)
}
}
}
### NIMBLE CANNOT SUM OVER 3 DIMENSIONS, hence the JAGS code does not work
# fit <- sum(E2[1:nsite,1:nrep,1:nyear])# Sum up squared residuals for actual data set
# fit.new <- sum(E2.new[1:nsite,1:nrep,1:nyear]) # Sum up for replicate data sets
## alternative solution from https://groups.google.com/g/nimble-users/c/fI8fXBpgIAE
for(year in 1:nyear){
for(i in 1:nsite){
fsum[i,year]<- sum(E2[i,1:nrep,year])
nsum[i,year] <- sum(E2.new[i,1:nrep,year])
}
}
fit <- sum(fsum[1:nsite,1:nyear])
fit.new <- sum(nsum[1:nsite,1:nyear])
}) ## end of nimble code chunk
######################################################################################################
########## CREATE INPUT DATA FOR NIMBLE - GENERIC PART     -----------------------
#######################################################################################################
#### data and inits that will be the same for all species
#### DISTINGUISH CONSTANTS AND DATA
# Constants are values that do not change, e.g. vectors of known index values or the indices used to define for loops
# Data are values that you might want to change, basically anything that only appears on the left of a ~
trend.constants <- list(nsite=nsites,
nrep=3,
primocc=seq(2011:YEAR),
nyear=nyears,
elev=siteCov$elev,
treeheight=siteCov$tree,
canopy=siteCov$canopy,
rain=rain,
wind=wind,
day=day,
ridge=siteCov$ridge,
time=time,
ACT=ACT)
####   DEFINE INITIAL VALUES----     ################################
## MUST BE FOR ALL PARAMETERS
## NIMBLE CAN HAVE CONVERGENCE PROBLEMS IF DIFFERENT INITS ARE SPECIFIED: https://groups.google.com/g/nimble-users/c/dgx9ajOniG8
inits.trend <- list(#N = Nst,
trend=runif(1,-2,2),
loglam = runif(1,-2,2),
sigma.site = runif(1,0,2),
sigma.year=runif(1,0,2),
sigma.p=runif(1,0,2),
beta.canopy=runif(1,-2,2),
#beta.rain=runif(1,-2,2),
beta.treeheight=runif(1,-2,2),
beta.elev=runif(1,-2,2),
bwind=-1,
brain=-1,
bridge=-1,
btime=-1,
b2time=-1,
bday=1,
bact=2,
p0 = runif(nyears,0.1,0.9))
inits.trend$lam.site<-rnorm(nsites,inits.trend$loglam,inits.trend$sigma.site)
inits.trend$lam.year<-rnorm(nyears,(inits.trend$trend*seq(1:(nyears))),inits.trend$sigma.year)
####   DEFINE RUN SETTINGS AND OUTPUT DATA----     ################################
# Define parameters to be monitored
parameters.trend <- c("fit", "fit.new","trend","totalN","anndet")  #
# MCMC settings
# number of posterior samples per chain is n.iter - n.burnin
n.iter <- 2500
n.burnin <- 1500
n.chains <- 3
# PRELIMINARY TEST OF NIMBLE MODEL TO IDENTIFY PROBLEMS --------------------
### fill in array for bird data and initial values
bird_s<-SURVEYDATA[,c(1,2,3,4,14)] %>%
arrange(Point,year,Count) %>%
rename(N=5) %>%
mutate(N=if_else(is.na(VisitID),NA,N)) %>%  ### RE-INTRODUCE THE NAs for COUNTS THAT DID NOT TAKE PLACE #####
dplyr::select(Point,year,Count,N)
BIRD.y<-array(NA, dim=c(nsites,3,nyears))
for (y in 2011:YEAR){
x<-bird_s %>%
dplyr::filter(year==y) %>%
dplyr::select(Point, Count, N) %>%
tidyr::spread(key=Count, value=N) %>%
dplyr::arrange(Point)
yc<-match(y,c(2011:YEAR))						## translates the year (2011, 2012, etc.) into consecutive number (1,2,...) for array dimensions
BIRD.y[,,yc]<-as.matrix(x[,2:4])
}
trend.data <- list(M = BIRD.y)
test <- nimbleModel(code = trend.model,
constants=trend.constants,
data = trend.data,
inits = inits.trend,
calculate=TRUE)
# USE TEST VALUES TO SUPPLEMENT INITS
inits.trend$lp = array(rnorm(trend.constants$nsite*trend.constants$nrep*trend.constants$nyear, c(test$mu.lp), inits.trend$sigma.p),
dim= c(trend.constants$nsite, trend.constants$nrep,trend.constants$nyear))
test$calculate()
# inits.trend$p0
# test$initializeInfo()
#
# # Missing values (NAs) or non-finite values were found in model variables:
# # M, p, lp, anndet,
# # E, E2, M.new, E.new, E2.new, fit, fit.new.
#
# ### make sure that none of the logProbs result in NA or -Inf as the model will not converge
# test$calculate()
# is.na(trend.constants) ## check whether there are NA in the data
# test$calculate(nodes="lam.site") # this seems to be ok
# test$logProb_lam.site
# test$logProb_lam.year
# test$logProb_p0
# log(test$logProb_p0/(1-test$logProb_p0))
# test$logProb_btime
# test$logProb_bday
# test$logProb_bridge
# test$logProb_bact
# test$logProb_M  # this is NA for all values
# test$logProb_lp  # this is NA for all values
# test$logProb_p  # this is NA for all values
# test$initializeInfo()
# #help(modelInitialization)
#
# ### make sure that none of the logProbs result in NA or -Inf as the model will not converge
# configureMCMC(test) # check that the samplers used are ok - all RW samplers need proper inits
s="MTOR"
######################################################################################
#############  TAKE SUBSET OF DATA FOR FOCAL SPECIES AND SORT THE TABLES    ###################
######################################################################################
bird_s<-SURVEYDATA[,c(1,2,3,4,match(s,colnames(SURVEYDATA)))] %>%
arrange(Point,year,Count) %>%
rename(N=5) %>%
mutate(N=if_else(is.na(VisitID),NA,N)) %>%  ### RE-INTRODUCE THE NAs for COUNTS THAT DID NOT TAKE PLACE #####
dplyr::select(Point,year,Count,N)
###############################################################################
############## CREATE BIRD DATA INPUT MATRIX   ################################
###############################################################################
#### FILL THE MISSING DATA WITH MEAN VALUES FOR INITS
## https://groups.google.com/g/nimble-users/c/wCwacQPLR2w?pli=1
### create array to be filled with data
BIRD.y<-array(NA, dim=c(nsites,3,nyears))
inits.y<-array(NA, dim=c(nsites,3,nyears))
inits.new<-array(NA, dim=c(nsites,3,nyears))
### fill in array for bird data and initial values
for (y in 2011:YEAR){
x<-bird_s %>%
dplyr::filter(year==y) %>%
dplyr::select(Point, Count, N) %>%
tidyr::spread(key=Count, value=N) %>%
dplyr::arrange(Point)
yc<-match(y,c(2011:YEAR))						## translates the year (2011, 2012, etc.) into consecutive number (1,2,...) for array dimensions
BIRD.y[,,yc]<-as.matrix(x[,2:4])
x<-bird_s %>%
mutate(N=ifelse(is.na(N),median(bird_s$N, na.rm=T),NA)) %>%   ### fill in missing values
dplyr::filter(year==y) %>%
dplyr::select(Point, Count, N) %>%
tidyr::spread(key=Count, value=N) %>%
dplyr::arrange(Point)
inits.y[,,yc]<-as.matrix(x[,2:4])
x<-bird_s %>%
mutate(N=ifelse(is.na(N),median(bird_s$N, na.rm=T),N)) %>%   ### fill in missing values
dplyr::filter(year==y) %>%
dplyr::select(Point, Count, N) %>%
tidyr::spread(key=Count, value=N) %>%
dplyr::arrange(Point)
inits.new[,,yc]<-as.matrix(x[,2:4])
}
#### GET THE MAXIMUM COUNT PER POINT PER YEAR FOR INITIAL VALUES
Nst<-as.matrix(bird_s %>%
mutate(N=ifelse(is.na(N),median(bird_s$N, na.rm=T),N)) %>%   ### fill in missing values - switch to max if there is invalid parent error
group_by(Point, year) %>%
summarise(K=max(N, na.rm=T)) %>%
spread(key=year,value=K, fill=max(bird_s$N,na.rm=T)) %>%
ungroup() %>%
arrange(Point) %>%
dplyr::select(-Point))
######################################################################################################
########## CREATE INPUT DATA FOR NIMBLE ------------------------
#######################################################################################################
#### DISTINGUISH CONSTANTS AND DATA
# Constants are values that do not change, e.g. vectors of known index values or the indices used to define for loops
# Data are values that you might want to change, basically anything that only appears on the left of a ~
trend.data <- list(M = BIRD.y)
####   ADD INITIAL VALUES----     ################################
## MUST ADD Nst TO INITIAL VALUESBE FOR ALL PARAMETERS
## NIMBLE CAN HAVE CONVERGENCE PROBLEMS IF DIFFERENT INITS ARE SPECIFIED: https://groups.google.com/g/nimble-users/c/dgx9ajOniG8
inits.trend$lp = array(rnorm(trend.constants$nsite*trend.constants$nrep*trend.constants$nyear, c(test$mu.lp), inits.trend$sigma.p),
dim= c(trend.constants$nsite, trend.constants$nrep,trend.constants$nyear))
inits.trend$N = Nst
inits.trend$M = inits.y
inits.trend$M.new = inits.new
allchaininits.trend <- list(inits.trend, inits.trend, inits.trend)
###############################################################################
####   RUN THE MODEL IN NIMBLE  --------------------###########################
###############################################################################
### this takes 3-5 hrs for 250000 iterations and converges for most species
TRENDMOD <- nimbleMCMC(code = trend.model,
constants=trend.constants,
data = trend.data,
inits = allchaininits.trend,
monitors = parameters.trend,
thin=4,
niter = n.iter,
nburnin = n.burnin,
nchains = n.chains,
progressBar = getNimbleOption("MCMCprogressBar"),
summary=T)
##### Montserrat Forest Bird Counts - data import and analysis with 'unmarked' #####
##### written in Sep 2024 by  Filibert Heim, filibert.heim@posteo.de          #####
##### 1: load required packages ####
# install.packages('scales')
library(scales)
# install.packages('data.table')
library(data.table)
library(reshape)
library(lubridate)
library(unmarked)
library(AICcmodavg) # package for model selection and goodness-of-fit tests
library(MuMIn)
library(tidyverse)
filter <- dplyr::filter
select <- dplyr::select
rename <- dplyr::rename
setwd("C:\\STEFFEN\\OneDrive - THE ROYAL SOCIETY FOR THE PROTECTION OF BIRDS\\STEFFEN\\RSPB\\UKOT\\Montserrat\\Analysis\\Population_status_assessment\\AnnualMonitoring\\Montserrat")
load(file = 'data/MONTSERRAT_ANNUAL_DATA_INPUT2024.RData') # change to the current year (most recent year with prepared data)
###### 2.1: set YEAR and SPECIES and rm_point that should be analysed ####
# YEAR <- 2024 # set the most recent year
SPECIES # all species prepared data is available for
SPECIES <- c('ACHU') # fill in SPECIES the analysis should be made for
rm_point<-c(99,76) # remove two points which are not independent
###### 2.2: check the data and remove unneeded stuff #####
head(countdata) # table with all birdcounts
head(obsCov) # all observation level Covs
rm(obsCov) # they are not needed since all information is available in countdata
head(siteCov) # site level Covs
(species_names <- species) # species codes and their English names
rm(species)
##### 3: prepare data for unmarkedMultFrame ####
###### 3.1: prepare siteCovs ####
head(siteCov)
siteCov <- siteCov %>%
rename_with(tolower) %>%
rename(canopy = canopy_cover, alt = elevation) %>%
filter(!point %in% rm_point) %>%  # remove all 2 points that are not independent
arrange(point)
###### 3.2: prepare obsCov #####
obsCov <- countdata %>%
select(year, Point, Count, Rain, Wind, day, time, activity) %>%
rename_with(tolower) %>%
filter(!point %in% rm_point) %>%
arrange(point, year, count)
###### 3.3: prepare yearlySiteCovs ####
yearlySiteCovYear <- obsCov %>%
group_by(point,year) %>%
summarise(season = as.numeric(mean(year) - 2010)) %>% # code year as.numeric
spread(key = year, value = season) %>% # spread a key-value pair across multiple columns
arrange(point)
yearlySiteCov <- list(year=yearlySiteCovYear[,2:ncol(yearlySiteCovYear)])
###### 3.3: prepare countdata ####
head(countdata)
countdata <- countdata %>% select(year, Point, Count, all_of(SPECIES)) %>% # select all the columns, including the correct species for analysis
rename(n = all_of(SPECIES)) %>% # this filters for the one species that
rename_with(tolower) %>%
filter(!point %in% rm_point) # remove the points that should not be analysed
occdata <- countdata %>%
mutate(occupancy = ifelse(n > 0, 1, 0)) %>% # convert in detection/non-detection
mutate(season = paste(year, count, sep = '_')) %>% # connect year, count to string
select(-n, -year, -count) %>% # remove unneeded columns
spread(key = season, value = occupancy) %>% # spread a key-value pair across multiple columns
arrange(point) # sort rows in order of point number
###### 3.4: prepare numPrimary (number of survey years) ####
numPrimary <- length(unique(countdata$year)) # calculates number of years
###### 3.6: check dimensions and input data.frames ####
head(siteCov)
head(obsCov)
head(yearlySiteCov)
head(occdata)
dim(occdata)
dim(siteCov)
nrow(obsCov)/(3*numPrimary)
dim(yearlySiteCovYear)
###### 3.3: create unmarkedMultFrame ####
umf <- unmarkedMultFrame(y = occdata[,2:ncol(occdata)], siteCovs = siteCov, yearlySiteCovs = yearlySiteCov,
obsCovs = obsCov, numPrimary = numPrimary)
summary(umf) # looks good
str(umf) # looks good, but pay attention: not all variables from this data set are coded in an appropriate way (as.factor())
siteCovs(umf)[c(2,4:6)] <- scale(siteCovs(umf)[c(2,4:6)]) # scale elevation, dbh and teeheight
obsCovs(umf)[c(1,4,6:8)] <- scale(obsCovs(umf)[c(1,4,6:8)]) # scale activity, rain, day and time (all numeric variables)
yearlySiteCovs(umf)[1] <- scale(yearlySiteCovs(umf)[1])
str(umf)
summary(umf)
global_model <- colext(~alt:treeheight+dbh, ~year+alt, ~year+alt, ~day+time+I(time^2)+rain+wind+activity+location, data = umf, se = T) # global model which is a year corrected shift model
gof <- mb.gof.test(global_model, nsim = 100, parallel = T) # perform gof with 1000 sim
gof
# check gof
print(gof)
c_hat <- gof$c.hat.est # save c-hat estimates for usage of the QAIC
p_value = gof$p.value # save p_value for modsel table
###### 4.2: fit models for detection probability p() first for modSel ####
# fit models for detection probability p() manually, go on with fitList(), modSel()
fm1 <- colext(~1, ~1, ~1, ~1, data = umf, se = T)
fm2 <- colext(~1, ~1, ~1, ~day, data = umf, se = T)
fm3 <- colext(~1, ~1, ~1, ~day+time, data = umf, se = T)
fm4 <- colext(~1, ~1, ~1, ~day+time+rain, data = umf, se = T)
fm5 <- colext(~1, ~1, ~1, ~day+time+rain+wind, data = umf, se = T)
fm6 <- colext(~1, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T)
fm7 <- colext(~1, ~1, ~1, ~day+time+rain+wind+activity+location, data = umf, se = T)
p_fitList <- list(fm1, fm2, fm3, fm4, fm5, fm6, fm7)
names(p_fitList) <- lapply(p_fitList, function(x) formula(x)) # set formulas as model names
(p_modSel_df <- aictab(cand.set = p_fitList, c.hat = c_hat) %>% # create a model comparison table with QAICc
mutate(step = 'p'))
# fit models for detection probability p() manually, go on with fitList(), modSel()
fm1 <- colext(~1, ~1, ~1, ~1, data = umf, se = T)
fm2 <- colext(~1, ~1, ~1, ~day, data = umf, se = T)
fm3 <- colext(~1, ~1, ~1, ~day+time+I(time^2), data = umf, se = T)
fm4 <- colext(~1, ~1, ~1, ~day+time+I(time^2)+rain, data = umf, se = T)
fm5 <- colext(~1, ~1, ~1, ~day+time+rain+wind, data = umf, se = T)
fm6 <- colext(~1, ~1, ~1, ~day+time+I(time^2)+rain+wind+activity, data = umf, se = T)
fm7 <- colext(~1, ~1, ~1, ~day+time+I(time^2)+rain+wind+activity+location, data = umf, se = T)
p_fitList <- list(fm1, fm2, fm3, fm4, fm5, fm6, fm7)
names(p_fitList) <- lapply(p_fitList, function(x) formula(x)) # set formulas as model names
(p_modSel_df <- aictab(cand.set = p_fitList, c.hat = c_hat) %>% # create a model comparison table with QAICc
mutate(step = 'p'))
fm6b <- colext(~1, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T)
p_fitList <- list(fm1, fm2, fm3, fm4, fm5, fm6, fm7, fm6b)
names(p_fitList) <- lapply(p_fitList, function(x) formula(x)) # set formulas as model names
(p_modSel_df <- aictab(cand.set = p_fitList, c.hat = c_hat) %>% # create a model comparison table with QAICc
mutate(step = 'p'))
###### 4.2: fit models for initial occupancy psi() first for modSel ####
fm8 <- colext(~alt, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T)
# add treeheight
fm9 <- colext(~treeheight, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T)
fm10 <- colext(~alt+treeheight, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T)
# add dbh
fm11 <- colext(~dbh, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T)
fm12 <- colext(~alt+dbh, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T)
fm13 <- colext(~treeheight+dbh, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T)
fm14 <- colext(~alt+treeheight+dbh, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T)
# add interaction between alt and treeheight
fm15 <- colext(~alt:treeheight, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T)
fm16 <- colext(~alt:treeheight+dbh, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T)
# put the fitted models in a fitList() and rank them by QAICc in modSel()
psi_fitList <- list(fm6, fm8, fm9, fm10, fm11, fm12, fm13, fm14, fm15, fm16) # don't forget to include the best model from the last modeling step!
names(psi_fitList) <- lapply(psi_fitList, function(x) formula(x)) # set formulas as model names
(psi_modSel_df <- aictab(cand.set = psi_fitList, c.hat = c_hat) %>%
mutate(step = 'psi'))
# best sub-model for psi(): ~1, QAICc difference to second best is 1.47 (~treeheight), then QAIC difference is 1.82 (~alt + treeheight) - go on with this best one (fm6)
###### 4.3: fit models for extinction and colonisation probability for modSel ####
fm17 <- colext(~1, ~1, ~1, ~day+time+rain+wind+activity, data = umf, se = T) # constant model
fm18 <- colext(~1, ~alt, ~1, ~day+time+rain+wind+activity, data = umf, se = T) # expansion model
fm19 <- colext(~1, ~1, ~alt, ~day+time+rain+wind+activity, data = umf, se = T) # contraction model
fm20 <- colext(~1, ~alt, ~alt, ~day+time+rain+wind+activity, data = umf, se = T) # shift model
fm21 <- colext(~1, ~year, ~year, ~day+time+rain+wind+activity, data = umf, se = T)  ## year model, this model will exclude the possibility that observed changes are just annual changes
fm22 <- colext(~alt+treeheight, ~year+alt, ~year, ~day+time+rain+wind+activity+location, data = umf, se = T)  # corrected year - expansion
fm23 <- colext(~alt+treeheight, ~year, ~year+alt, ~day+time+rain+wind+activity+location, data = umf, se = T)  # corrected year - contraction
fm24 <- colext(~alt+treeheight, ~year+alt, ~year+alt, ~day+time+rain+wind+activity+location, data = umf, se = T)  # corrected year - shift, also  global model
# put the fitted models in a list and rank them by QAIC in aictab
g_e_fitList <- list(constant = fm17, expansion = fm18, contraction = fm19,
shift = fm20, year = fm21, nullmodel = fm1,
year_expansion = fm22, year_contraction = fm23, year_shift = fm24)
names(g_e_fitList) <- lapply(g_e_fitList, function(x) formula(x)) # set formulas as model names
model_names <- aictab(cand.set = list(constant = fm17, expansion = fm18, contraction = fm19, shift = fm20, year = fm21, nullmodel = fm1, year_expansion = fm22, year_contraction = fm23, year_shift = fm24), c.hat = c_hat)[, 1] # get model names in the correct order
(g_e_modSel_df <- aictab(cand.set = g_e_fitList, c.hat = c_hat) %>%
mutate(step = 'g_e',  # Add the step indicator
model = model_names))  # include short model names
chisq <- function(fm) {
umf <- getData(fm)
y <- getY(umf)
sr <- fm@sitesRemoved
if(length(sr)>0)
y <- y[-sr,,drop=FALSE]
fv <- fitted(fm, na.rm=TRUE)
y[is.na(fv)] <- NA
sum((y-fv)^2/(fv*(1-fv)))
}
pb.gof <- parboot(global_model, statistic=chisq, nsim=100)
pb.gof
pb.gof
fitstats <- function(global_model) {
observed <- getY(global_model@data)
expected <- fitted(global_model)
resids <- residuals(global_model)
sse <- sum(resids^2,na.rm=TRUE)
chisq <- sum((observed - expected)^2 / expected,na.rm=TRUE)
freeTuke <- sum((sqrt(observed) - sqrt(expected))^2,na.rm=TRUE)
out <- c(SSE=sse, Chisq=chisq, freemanTukey=freeTuke)
return(out)
}
pb <- parboot(global_model, fitstats, nsim=20, report=1)
cHat_pb <- pb@t0[2] / mean(p...@t.star[,2])
pb
