#############  Load required packages       ##########################################
######################################################################################
library(tidyverse)
library(data.table)
library(dplyr)
library(dtplyr)
library(lubridate)
library(ggplot2)
# library(knitr)
# library(rmarkdown)
library(MCMCvis)
library(nimble)
library(basicMCMCplots) # for trace plots called chainsPlot
library(parallel)
library(foreach)
library(doParallel)
######################################################################################
#############  Set your working directory (path where the database is)       #########
######################################################################################
#setwd("C:\\STEFFEN\\RSPB\\Montserrat\\Analysis\\Population_status_assessment\\AnnualMonitoring\\Montserrat")
setwd("C:\\STEFFEN\\OneDrive - THE ROYAL SOCIETY FOR THE PROTECTION OF BIRDS\\STEFFEN\\RSPB\\UKOT\\Montserrat\\Analysis\\Population_status_assessment\\AnnualMonitoring\\Montserrat")
#setwd("C:\\Users\\sop\\Documents\\Steffen\\RSPB\\Montserrat\\Montserrat")
######################################################################################
#############  load the pre-prepared dataset					     #########
######################################################################################
load("data/MONTSERRAT_ANNUAL_DATA_INPUT2024.RData")
#load("S:\\ConSci\\DptShare\\SteffenOppel\\RSPB\\Montserrat\\Analysis\\Population_status_assessment\\AnnualMonitoring\\MONTSERRAT_ANNUAL_DATA_INPUT2023.RData")
fullnames<-c("Montserrat Oriole", "Forest Thrush", "Bridled Quail-Dove", "Brown Trembler",
"Antillean Crested Hummingbird","Purple-throated Carib",
"Pearly-eyed Thrasher","Green-throated Carib","Scaly-breasted Thrasher","Scaly-naped Pigeon",
"Caribbean Elaenia","Bananaquit")
### define dimensions of arrays
nsites<-length(unique(siteCov$Point))
nyears<-length(unique(countdata$year))
### removed on 8 Oct 2024 because it did not explain anything
# ### summarise rainfall from Jan to March, productivity from PREVIOUS year will affect count in current year
# rain<-fread("data/MontserratRain2005_2023.csv",fill=TRUE) %>%
#   dplyr::filter(Variable=="RainMM") %>%
#   dplyr::filter(YEAR %in% seq(2010,2024,1)) %>%
#   dplyr::select(-Variable,-Total) %>%
#   gather(key="Month", value="mm",-YEAR) %>%
#   dplyr::filter(Month %in% c('JAN','FEB','MAR')) %>%
#   group_by(YEAR) %>%
#   summarise(rain=sum(mm)) %>%
#   mutate(rain=scale(rain)[,1])
###############################################################################
############## CREATE SITE COVARIATE DATA INPUT MATRIX   ######################
###############################################################################
siteCov<-siteCov %>% arrange(Point) %>%
mutate(ridge=ifelse(Location=="Ridge",1,0)) %>%
dplyr::select(Point,treeheight,Elevation,Canopy_cover,ridge) %>%
mutate(tree=scale(treeheight)[,1], elev=scale(Elevation)[,1],canopy=scale(Canopy_cover)[,1])
###############################################################################
############## CREATE OBSERVATION COVARIATE DATA INPUT MATRIX   ###############
###############################################################################
SURVEYDATA<-countdata %>%
arrange(Point,year,Count) %>%
mutate(activity=ifelse(is.na(activity),mean(activity, na.rm=T),activity)) %>%
mutate(time=scale(time),day=scale(day),activity=scale(activity))
### only needs standardisation if measured in mm, not as 0/1 variable
#meant<-mean(SURVEYDATA$rain, na.rm = TRUE)
#sdt<-sd(SURVEYDATA$rain, na.rm = TRUE)
#SURVEYDATA$rain<-(SURVEYDATA$rain-meant)/sdt
### create array for each covariate
wind<-array(NA, dim=c(nsites,3,nyears))
rain<-array(NA, dim=c(nsites,3,nyears))
time<-array(NA, dim=c(nsites,3,nyears))
day<-array(NA, dim=c(nsites,3,nyears))
ACT<-array(NA, dim=c(nsites,3,nyears))				## REPLACED ON 2 MAY WITH RAINFALL AMOUNT
### fill in array for each covariate
for (y in 2011:YEAR){
obsC<-subset(SURVEYDATA, year==y)
y<-match(y,c(2011:YEAR))						## translates the year (2011, 2012, etc.) into consecutive number (1,2,...) for array dimensions
x<-obsC %>% dplyr::select(Point, Count, time) %>% tidyr::spread(key=Count, value=time) %>% dplyr::arrange(Point)
time[,,y]<-as.matrix(x[,2:4])
x<-obsC %>% dplyr::select(Point, Count, day) %>% tidyr::spread(key=Count, value=day) %>% dplyr::arrange(Point)
day[,,y]<-as.matrix(x[,2:4])
x<-obsC %>% dplyr::select(Point, Count, Wind) %>%
mutate(Wind=ifelse(Wind<3,0,1)) %>%
tidyr::spread(key=Count, value=Wind) %>% dplyr::arrange(Point)
wind[,,y]<-as.matrix(x[,2:4])
x<-obsC %>% dplyr::select(Point, Count, activity) %>% tidyr::spread(key=Count, value=activity) %>% dplyr::arrange(Point)
ACT[,,y]<-as.matrix(x[,2:4])
x<-obsC %>% dplyr::select(Point, Count, Rain) %>% tidyr::spread(key=Count, value=Rain) %>% dplyr::arrange(Point)
rain[,,y]<-as.matrix(x[,2:4])
}
###############################################################################
####   REPLACE ALL NA IN COVARIATES otherwise "undefined node" error    #######
###############################################################################
for (d in 1:nyears){							### replace missing dates with mean for each survey round in each year
ACT[is.na(ACT[,1,d]),1,d]<-mean(ACT[,1,d], na.rm=T)
ACT[is.na(ACT[,2,d]),2,d]<-mean(ACT[,2,d], na.rm=T)
ACT[is.na(ACT[,3,d]),3,d]<-mean(ACT[,3,d], na.rm=T)
time[is.na(time[,1,d]),1,d]<-mean(time[,1,d], na.rm=T)
time[is.na(time[,2,d]),2,d]<-mean(time[,2,d], na.rm=T)
time[is.na(time[,3,d]),3,d]<-mean(time[,3,d], na.rm=T)
day[is.na(day[,1,d]),1,d]<-mean(day[,1,d], na.rm=T)
day[is.na(day[,2,d]),2,d]<-mean(day[,2,d], na.rm=T)
day[is.na(day[,3,d]),3,d]<-mean(day[,3,d], na.rm=T)
wind[is.na(wind[,1,d]),1,d]<-0
wind[is.na(wind[,2,d]),2,d]<-0
wind[is.na(wind[,3,d]),3,d]<-0
rain[is.na(rain[,1,d]),1,d]<-0
rain[is.na(rain[,2,d]),2,d]<-0
rain[is.na(rain[,3,d]),3,d]<-0
}
seq(2011:YEAR)
######################################################################################
#############  WRITE THE NIMBLE MODEL AND SET INITS  ############################
######################################################################################
# Specify model in NIMBLE format
trend.model<-nimbleCode({
####  Priors ########
loglam~dunif(-5,5)          ##  mean abundance prior
trend~dunif(-5,5)         ##  trend prior
beta.elev~dunif(-5,5)
#beta.rain~dunif(-2,2)
beta.canopy~dunif(-5,5)
beta.treeheight~dunif(-5,5)
bwind~dunif(-5,0)   ## wind can only have negative effect on detection
brain~dunif(-5,0)   ## rain can only have negative effect on detection
btime~dunif(-5,5)
b2time~dunif(-5,5)
bday~dunif(-5,5)
bridge~dunif(-5,5)
bact~dunif(-5,5)
## SITE RANDOM EFFECT ##
for(i in 1:nsite){
lam.site[i]~dnorm(loglam,tau=tau.site)    ## site-specific random effect with hierarchical centering from Kery email 5 June 2018
}
tau.site<-1/(sigma.site*sigma.site)
sigma.site~dunif(0,2)
## YEAR RANDOM EFFECT FOR ABUNDANCE AND ANNUALLY VARYING DETECTION PROBABILITY ##
for(year in 1:nyear){
p0[year]~dunif(0.01,0.99)## detection probability
logitp0[year]<-log(p0[year]/(1-p0[year]))
#lam.year[year]~dnorm(trend*primocc[year],tau=tau.year)    ## year-specific random effect with hierarchical centering from Kery email 5 June 2018
lam.year[year]~dnorm(loglam,tau=tau.year)    ## year-specific random effect with hierarchical centering from Kery email 5 June 2018
}
tau.year<-1/(sigma.year*sigma.year)
sigma.year~dunif(0,2)
######### State and observation models ##############
for(year in 1:nyear){
for(i in 1:nsite){
log(lambda[i,year])<- lam.year[year]+
trend*primocc[year]+
beta.elev*elev[i]+
beta.treeheight*treeheight[i]+
beta.canopy*canopy[i]+
lam.site[i]
N[i,year]~dpois(lambda[i,year])
for(t in 1:nrep){
M[i,t,year]~dbin(p[i,t,year],N[i,year])
#p[i,t,year] <- exp(lp[i,t,year])/(1+exp(lp[i,t,year]))
#lp[i,t,year] ~ dnorm(mu.lp[i,t,year], tau=tau.lp)
logit(p[i,t,year])<-logitp0[year] +
btime*time[i,t,year]+
b2time * pow(time[i,t,year], 2) +
bday*day[i,t,year]+
bridge*ridge[i]+
bwind*wind[i,t,year]+
brain*rain[i,t,year]+
bact*ACT[i,t,year]
}
}
### DERIVED PARAMETER FOR EACH YEAR ###
totalN[year]<-sum(N[1:nsite,year])
anndet[year]<-mean(p[1:nsite,1:nrep,year])
}
# Computation of fit statistic (Bayesian p-value)
# Fit statistic for observed data
# Also, generate replicate data and compute fit stats for them
for(year in 1:nyear){
for(i in 1:nsite){
for(t in 1:nrep){
# Actual data
eval[i,t,year] <-N[i,year]*p[i,t,year] # Expected value
sd.resi[i,t,year]<-sqrt(eval[i,t,year]*(1-p[i,t,year])) +0.5
E[i,t,year]<-(M[i,t,year]-eval[i,t,year])/ sd.resi[i,t,year]
E2[i,t,year] <- pow(E[i,t,year],2)
# Replicate data sets
M.new[i,t,year]~dbin(p[i,t,year],N[i,year])
E.new[i,t,year]<-(M.new[i,t,year]-eval[i,t,year])/sd.resi[i,t,year]
E2.new[i,t,year] <- pow(E.new[i,t,year], 2)
}
}
}
### NIMBLE CANNOT SUM OVER 3 DIMENSIONS, hence the JAGS code does not work
# fit <- sum(E2[1:nsite,1:nrep,1:nyear])# Sum up squared residuals for actual data set
# fit.new <- sum(E2.new[1:nsite,1:nrep,1:nyear]) # Sum up for replicate data sets
## alternative solution from https://groups.google.com/g/nimble-users/c/fI8fXBpgIAE
for(year in 1:nyear){
for(i in 1:nsite){
fsum[i,year]<- sum(E2[i,1:nrep,year])
nsum[i,year] <- sum(E2.new[i,1:nrep,year])
}
}
fit <- sum(fsum[1:nsite,1:nyear])
fit.new <- sum(nsum[1:nsite,1:nyear])
}) ## end of nimble code chunk
trend.constants <- list(nsite=nsites,
nrep=3,
primocc=seq(2011:YEAR),
nyear=nyears,
elev=siteCov$elev,
treeheight=siteCov$tree,
canopy=siteCov$canopy,
rain=rain,
wind=wind,
day=day,
ridge=siteCov$ridge,
time=time,
ACT=ACT)
inits.trend <- list(#N = Nst,
trend=runif(1,-2,2),
loglam = runif(1,-2,2),
sigma.site = runif(1,0,2),
sigma.year=runif(1,0,2),
#sigma.p=runif(1,0,2),
beta.canopy=runif(1,-2,2),
#beta.rain=runif(1,-2,2),
beta.treeheight=runif(1,-2,2),
beta.elev=runif(1,-2,2),
bwind=-1,
brain=-1,
bridge=-1,
btime=-1,
b2time=-1,
bday=1,
bact=2,
p0 = runif(nyears,0.1,0.9))
inits.trend$lam.site<-rnorm(nsites,inits.trend$loglam,inits.trend$sigma.site)
inits.trend$lam.year<-rnorm(nyears,inits.trend$loglam,inits.trend$sigma.year)
# Define parameters to be monitored
parameters.trend <- c("fit", "fit.new","trend","totalN","anndet")  #
# MCMC settings
# number of posterior samples per chain is n.iter - n.burnin
n.iter <- 5000
n.burnin <- 2500
n.chains <- 3
# PRELIMINARY TEST OF NIMBLE MODEL TO IDENTIFY PROBLEMS --------------------
### fill in array for bird data and initial values
bird_s<-SURVEYDATA[,c(1,2,3,4,14)] %>%
arrange(Point,year,Count) %>%
rename(N=5) %>%
mutate(N=if_else(is.na(VisitID),NA,N)) %>%  ### RE-INTRODUCE THE NAs for COUNTS THAT DID NOT TAKE PLACE #####
dplyr::select(Point,year,Count,N)
BIRD.y<-array(NA, dim=c(nsites,3,nyears))
for (y in 2011:YEAR){
x<-bird_s %>%
dplyr::filter(year==y) %>%
dplyr::select(Point, Count, N) %>%
tidyr::spread(key=Count, value=N) %>%
dplyr::arrange(Point)
yc<-match(y,c(2011:YEAR))						## translates the year (2011, 2012, etc.) into consecutive number (1,2,...) for array dimensions
BIRD.y[,,yc]<-as.matrix(x[,2:4])
}
trend.data <- list(M = BIRD.y)
test <- nimbleModel(code = trend.model,
constants=trend.constants,
data = trend.data,
inits = inits.trend,
calculate=TRUE)
# USE TEST VALUES TO SUPPLEMENT INITS
inits.trend$lp = array(rnorm(trend.constants$nsite*trend.constants$nrep*trend.constants$nyear, c(test$mu.lp), inits.trend$sigma.p),
dim= c(trend.constants$nsite, trend.constants$nrep,trend.constants$nyear))
test$calculate()
test$logProb_p0
test$logProb_lam.site
test$logProb_lam.year
test$initializeInfo()
test$logProb_btime
test$logProb_bday
test$logProb_bridge
test$logProb_bact
test$logProb_M
test$logProb_p
inits.trend$p0
inits.trend$p = array(rnorm(trend.constants$nsite*trend.constants$nrep*trend.constants$nyear, c(inits.trend$p0), inits.trend$sigma.p),
dim= c(trend.constants$nsite, trend.constants$nrep,trend.constants$nyear))
inits.trend$p = array(runif(trend.constants$nsite*trend.constants$nrep*trend.constants$nyear, 0.3,0.7),
dim= c(trend.constants$nsite, trend.constants$nrep,trend.constants$nyear))
test$calculate()
test$initializeInfo()
test$logProb_p
test$logProb_M
s="MTOR"
bird_s<-SURVEYDATA[,c(1,2,3,4,match(s,colnames(SURVEYDATA)))] %>%
arrange(Point,year,Count) %>%
rename(N=5) %>%
#mutate(N=if_else(is.na(VisitID),NA,N)) %>%  ### RE-INTRODUCE THE NAs for COUNTS THAT DID NOT TAKE PLACE #####
dplyr::select(Point,year,Count,N)
###############################################################################
############## CREATE BIRD DATA INPUT MATRIX   ################################
###############################################################################
#### FILL THE MISSING DATA WITH MEAN VALUES FOR INITS
## https://groups.google.com/g/nimble-users/c/wCwacQPLR2w?pli=1
### create array to be filled with data
BIRD.y<-array(NA, dim=c(nsites,3,nyears))
inits.y<-array(NA, dim=c(nsites,3,nyears))
inits.new<-array(NA, dim=c(nsites,3,nyears))
### fill in array for bird data and initial values
for (y in 2011:YEAR){
x<-bird_s %>%
dplyr::filter(year==y) %>%
dplyr::select(Point, Count, N) %>%
tidyr::spread(key=Count, value=N) %>%
dplyr::arrange(Point)
yc<-match(y,c(2011:YEAR))						## translates the year (2011, 2012, etc.) into consecutive number (1,2,...) for array dimensions
BIRD.y[,,yc]<-as.matrix(x[,2:4])
x<-bird_s %>%
mutate(N=ifelse(is.na(N),median(bird_s$N, na.rm=T),N)) %>%   ### fill in missing values
dplyr::filter(year==y) %>%
dplyr::select(Point, Count, N) %>%
tidyr::spread(key=Count, value=N) %>%
dplyr::arrange(Point)
inits.y[,,yc]<-as.matrix(x[,2:4])
inits.new[,,yc]<-as.matrix(x[,2:4])
}
#### GET THE MAXIMUM COUNT PER POINT PER YEAR FOR INITIAL VALUES
Nst<-as.matrix(bird_s %>%
mutate(N=ifelse(is.na(N),median(bird_s$N, na.rm=T),N)) %>%   ### fill in missing values - switch to max if there is invalid parent error
group_by(Point, year) %>%
summarise(K=max(N, na.rm=T)) %>%
spread(key=year,value=K, fill=max(bird_s$N,na.rm=T)) %>%
ungroup() %>%
arrange(Point) %>%
dplyr::select(-Point))
######################################################################################################
########## CREATE INPUT DATA FOR NIMBLE ------------------------
#######################################################################################################
#### DISTINGUISH CONSTANTS AND DATA
# Constants are values that do not change, e.g. vectors of known index values or the indices used to define for loops
# Data are values that you might want to change, basically anything that only appears on the left of a ~
trend.data <- list(M = BIRD.y)
inits.trend$N = Nst
inits.trend$M = inits.y
inits.trend$M.new = inits.new
test <- nimbleModel(code = trend.model,
constants=trend.constants,
data = trend.data,
inits = inits.trend,
calculate=TRUE)
test$calculate()
allchaininits.trend <- list(inits.trend, inits.trend, inits.trend)
###############################################################################
####   RUN THE MODEL IN NIMBLE  --------------------###########################
###############################################################################
### this takes 3-5 hrs for 250000 iterations and converges for most species
TRENDMOD <- nimbleMCMC(code = trend.model,
constants=trend.constants,
data = trend.data,
inits = allchaininits.trend,
monitors = parameters.trend,
thin=4,
niter = n.iter,
nburnin = n.burnin,
nchains = n.chains,
progressBar = getNimbleOption("MCMCprogressBar"),
summary=T)
# load packages
library(RODBC)
library(tidyverse)
select <- dplyr::select
rename <- dplyr::rename
filter <- dplyr::filter
# fill in the most recent year and set the species you want to analyse
YEAR <- 2024
SPECIES<-c('MTOR','FOTH','BRQD','TREM','ACHU','PTCA','PETH','GTCA','SBTH','SNPI','CAEL','BANA')
removal<-c(99,76)																			## removes only two points that cause error in JAGS
# set working directory
getwd()
#setwd('C:/Users/filib/Documents/Praktika/Sempach/Montserrat') # for Filibert
setwd('C:/STEFFEN/OneDrive - THE ROYAL SOCIETY FOR THE PROTECTION OF BIRDS/STEFFEN/RSPB/UKOT/Montserrat/Analysis/Population_status_assessment/AnnualMonitoring/Montserrat') # for Steffen
#### 2: load data from ms access database ####
# connect with database to load data from query and afterwards close connection again
db <- odbcConnectAccess2007('data/Montserrat_Birds_2024.accdb') # change name of the db to the actual one
tblVisit <- sqlFetch(db, 'tblVisit') # observation level covariates
birds <- sqlFetch(db, 'tblBirdData') # bird count data
Point_hab <- sqlFetch(db, 'point_habitat_data')
Point_tree <- sqlFetch(db, 'point_tree_data')
species <- sqlFetch(db, 'lkSpecies')
odbcClose(db)
save.image("data/Montserrat_Birds_2024_accdb.RData")
#load("data/Montserrat_Birds_2024_accdb.RData")
#### 3: prepare siteCovs ####
# first manipulate/aggregate the tree data measured for 4 trees near each point by averaging to one value for each point
head(Point_tree)
tree <- Point_tree %>% rename(Point = Point_ID, treeheight = Tree_height) %>%
group_by(Point) %>%
mutate(DBH = mean(DBH), # take the mean for each point
treeheight = mean(treeheight)) %>%
ungroup() %>%
select(Point, DBH, treeheight) %>% # remove all unneeded variables
distinct() # remove duplicate rows
# join the two siteCov data frames for easier handling and afterwards remove all unneeded variables/columns
siteCov <- Point_hab %>%
rename(Point = Point_ID) %>%
left_join(tree, by="Point") %>%
select(Point, Elevation, Location, DBH, treeheight,Canopy_cover) # select the habitat variables that are relevant
siteCov <- siteCov %>%
mutate(Point = as.numeric(Point),
Location = factor(Location)) # location as factor with 4 levels flat-open, midslope, ridge and valley
##### 3: prepare obsCov ####
# remove point visits that are very obviously not needed (another survey protocol before 2011)
head(tblVisit)
tblVisit <- tblVisit %>%
filter(year %in% 2011:YEAR) %>% # remove all surveys which were done in another sampling scheme before 2011
filter(Point %in% siteCov$Point) # remove all points that are not in the center hills and/or we don't have siteCovs
dim(tblVisit)
# tidy everything a bit up, remove unneeded columns and give the remaining a new order
head(tblVisit)
obsCov <- tblVisit %>%
select(-Redundant, -Cloud, -Observers, -NumObservers, -Route, -season, -GlobalID, -CensusID, -day, -month, -jday) %>% # remove unneeded columns
select(VisitID, year, Point, Count, everything()) %>% # reorder columns
arrange(year, Point, Count)
# store the existing ones in the proper format
str(obsCov)
obsCov <- obsCov %>%
mutate(Point = as.numeric(Point),
Rain = if_else(is.na(Rain), true = 0, false = 1), # assigns 0 for no rain and 1 for rain
Wind = recode(Wind, 'calm' = 1, 'light' = 2, 'moderate' = 3, 'strong' = 4) %>%
factor(levels = c(1, 2, 3, 4), ordered = TRUE))
# calculate missing variables
obsCov <- obsCov %>%
mutate(march1st = as.POSIXct(paste0(year, '-03-01', format = '%Y-%m-%d')), # calculate difference to the first of March (therefore create a dummy variable)
day = round(as.numeric(difftime(Date, march1st, units = 'days')), digits = 0), # calculate diff between dummy variable and Date
time = as.numeric(difftime(Time, min(Time), units = 'mins'))) %>% # calculate time difference to the min time
select(-march1st) # delete dummy variable
# check if all surveys were done
obsCov %>% group_by(year, Point) %>% summarise(number_counts = length(Count)) %>% filter(number_counts != 3) %>% ungroup()
# there are cases (23) where not all surveys were done in each year and point - include missing surveys by joining a df that includes all theoretically performed surveys to join with actual obsCov
# create df with all surveys that should have been done each year for each point
obsCov_theor <- data.frame(year = rep(2011:YEAR, each = length(siteCov$Point)),
Point = rep(siteCov$Point, each = 3, times = length(2011:YEAR)),
Count = rep(1:3, times = length(2011:YEAR)*length(siteCov$Point)))
# calculate the number of obsCovs / surveys that should theoretically be done
length(siteCov$Point)*3*length(2011:YEAR) # this is the calculation where 2020 is still present (but no surveys were performed because of COVID)
# join theoretical df obsCov_theor with actual performed survey obsCov to get information of missing surveys
obsCov <- obsCov_theor %>%
left_join(obsCov, by = c('year', 'Point', 'Count'))
## find non-existing surveys and replace covariates with mean (irrelevant for modelling)
obsCov$Rain[is.na(obsCov$Rain)]<-0
obsCov$Wind[is.na(obsCov$Wind)]<-2   ## check the most common valuetable(obsCov$Wind)
obsCov$day[is.na(obsCov$day)]<-median(obsCov$day, na.rm=T)
obsCov$time[is.na(obsCov$time)]<-median(obsCov$time, na.rm=T)
summary(obsCov$time)
#### 4: prepare bird countdata ####
##### digression to calculate activity covariate for obsCov BEFORE ALL OTHER SPECIES ARE ELIMINATED #####
ACT<-birds %>%
group_by(VisitID) %>%
summarise(activity = sum(Number,na.rm=T)) %>%
filter(!is.na(VisitID))
dim(ACT)
ACT %>% filter(is.na(activity))
# first, check for duplicate entries in the data base bird count data for each species - here we will only care for all data between 2011 and the most recent year and SPECIES
# birds %>%
#   filter(VisitID %in% obsCov$VisitID, Species %in% SPECIES) %>% # remove all observations from before 2011 which were done with another sampling scheme
#   group_by(VisitID, Species) %>%
#   summarise(n=length(Number)) %>%
#   filter(n>1) %>%
#   filter(!is.na(VisitID)) %>%
#   ungroup()
# okay, there are several duplicates to deal with - NEED TO BE SUMMED UP DUE TO HISTORIC DATA ENTRY (distance sampling)
# sum the numbers up, because a distance sampling framework was applied until 2011 and the duplicate entries refer to different distance bands, for all other cases: we cant solve it so we treat them the same
## create a matrix with 0 counts
birdmat <- birds %>%
filter(VisitID %in% obsCov$VisitID, Species %in% SPECIES) %>% # remove all observations from before 2011 which were done with another sampling scheme
group_by(VisitID, Species) %>%
summarise(Number = sum(Number)) %>% filter(!is.na(VisitID)) %>%
ungroup() %>%
spread(key=Species, value=Number, fill=0)
# create df where for each species obsCov are prepared to fill in birddata using a join
# obsCov_spec <- bind_rows(replicate(length(SPECIES), obsCov, simplify = FALSE)) %>%
#   mutate(Species = rep(SPECIES, each = nrow(obsCov)))
# head(obsCov_spec)
# bring both obsCov and bird data together
# countdata <- birds %>%
#   filter(VisitID %in% obsCov$VisitID, Species %in% SPECIES) %>% # remove all observations from before 2011 which were done with another sampling scheme
#   select(VisitID, Species, Number) %>% # select only the relevant columns
#   right_join(y = obsCov_spec, by = c('VisitID', 'Species')) %>%
#   arrange(Species, year, Point, Count) %>%
#   mutate(Number = ifelse(is.na(Number), 0, Number), Number = ifelse(is.na(VisitID), NA, Number)) %>%  # fill in 0 for counts where no birds of the species were seen
#   select(VisitID, Species, year, Point, Count, Number)
countdata<-obsCov %>%
left_join(ACT, by="VisitID") %>%
left_join(birdmat, by="VisitID")
dim(countdata)
dim(obsCov)
# calculate the theoretically number of observations that should have been done since 2011 at the selected points
length(siteCov$Point)*3*length(2011:YEAR)
#### 5: check prepared data ####
# siteCovs
head(siteCov)
dim(siteCov) # there are 88 points and 4 siteCovs as variables
# obsCov
head(obsCov)
dim(obsCov) # there are number of points times number of replictes/counts each year per point times number of survey years observations - check (next line)
nrow(obsCov) == length(siteCov$Point)*3*length(2011:YEAR) # must be TRUE
obsCov %>% group_by(year, Point) %>% summarise(number_counts = length(Count)) %>% filter(number_counts != 3) %>% ungroup() %>% nrow() # must be 0
# countdata
head(countdata)
nrow(countdata) == length(siteCov$Point)*3*length(2011:YEAR)# should be TRUE
unique(countdata$Species) # should contain all Species you want to study
countdata %>% filter(is.na(VisitID))
#### 6: save workspace for analysis ####
# fill in the objects you want to save from the environment for further analysis
needed_objects <- c('countdata', 'obsCov', 'siteCov', 'species', 'YEAR', 'SPECIES')
rm(list = setdiff(ls(), needed_objects)) # remove all unneeded objects
# save workspace to wd
#setwd() # set the file you want to store the data if it differs from the current wd
save.image(paste0('data/MONTSERRAT_ANNUAL_DATA_INPUT', YEAR, '.RData'))
